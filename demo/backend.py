import openai
import json
import os
import shutil
import re
import io
import contextlib
import traceback
from pathlib import Path
from urllib.parse import quote
import subprocess
import sys
import tempfile
import requests
import threading
import http.server
from functools import partial
import socketserver
import sqlite3
from fastapi import FastAPI, File, UploadFile, Form, HTTPException, Query, Body
from fastapi.responses import JSONResponse, Response, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from typing import List, Optional, Dict, Tuple
from collections import defaultdict
import httpx
import uvicorn
import os
import re
import json
from copy import deepcopy

PROJECT_ROOT = Path(__file__).resolve().parents[1]
API_DIR = PROJECT_ROOT / "API"
for path_candidate in (str(PROJECT_ROOT), str(API_DIR)):
    if path_candidate not in sys.path:
        sys.path.insert(0, path_candidate)

import config as api_config

os.environ.setdefault("MPLBACKEND", "Agg")


def execute_code(code_str):
    import io
    import contextlib
    import traceback

    stdout_capture = io.StringIO()
    stderr_capture = io.StringIO()
    try:
        with contextlib.redirect_stdout(stdout_capture), contextlib.redirect_stderr(
            stderr_capture
        ):
            exec(code_str, {})
        output = stdout_capture.getvalue()
        if stderr_capture.getvalue():
            output += stderr_capture.getvalue()
        return output
    except Exception as exec_error:
        code_lines = code_str.splitlines()
        tb_lines = traceback.format_exc().splitlines()
        error_line = None
        for line in tb_lines:
            if 'File "<string>", line' in line:
                try:
                    line_num = int(line.split(", line ")[1].split(",")[0])
                    error_line = line_num
                    break
                except (IndexError, ValueError):
                    continue
        error_message = f"Traceback (most recent call last):\n"
        if error_line is not None and 1 <= error_line <= len(code_lines):
            error_message += f'  File "<string>", line {error_line}, in <module>\n'
            error_message += f"    {code_lines[error_line-1].strip()}\n"
        error_message += f"{type(exec_error).__name__}: {str(exec_error)}"
        if stderr_capture.getvalue():
            error_message += f"\n{stderr_capture.getvalue()}"
        return f"[Error]:\n{error_message.strip()}"


def execute_code_safe(
    code_str: str, workspace_dir: str = None, timeout_sec: int = 120
) -> str:
    """åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­æ‰§è¡Œä»£ç ï¼Œæ”¯æŒè¶…æ—¶ï¼Œé¿å…é˜»å¡ä¸»è¿›ç¨‹ã€‚"""
    if workspace_dir is None:
        workspace_dir = WORKSPACE_BASE_DIR
    exec_cwd = os.path.abspath(workspace_dir)
    os.makedirs(exec_cwd, exist_ok=True)
    tmp_path = None
    try:
        fd, tmp_path = tempfile.mkstemp(suffix=".py", dir=exec_cwd)
        os.close(fd)
        with open(tmp_path, "w", encoding="utf-8") as f:
            f.write(code_str)
        print(
            f"[exec] Running script: {tmp_path} (timeout={timeout_sec}s) cwd={exec_cwd}"
        )
        # åœ¨å­è¿›ç¨‹ä¸­è®¾ç½®æ— ç•Œé¢ç¯å¢ƒå˜é‡ï¼Œé¿å… GUI åç«¯
        child_env = os.environ.copy()
        child_env.setdefault("MPLBACKEND", "Agg")
        child_env.setdefault("QT_QPA_PLATFORM", "offscreen")
        child_env.pop("DISPLAY", None)

        completed = subprocess.run(
            [sys.executable, tmp_path],
            cwd=exec_cwd,
            capture_output=True,
            text=True,
            timeout=timeout_sec,
            env=child_env,
        )
        output = (completed.stdout or "") + (completed.stderr or "")
        return output
    except subprocess.TimeoutExpired:
        return f"[Timeout]: execution exceeded {timeout_sec} seconds"
    except Exception as e:
        return f"[Error]: {str(e)}"
    finally:
        try:
            if tmp_path and os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass


# API endpoint and model path
API_BASE = "http://localhost:8000/v1"  # this localhost is for vllm api, do not change
MODEL_PATH = "qwen2.5-3b-instruct"  # replace to your path to DeepAnalyze-8B
MAX_ITERATIONS = 12
ANSWER_MIN_EXEC_ROUNDS = 3
ANSWER_MIN_NON_SCHEMA_ROUNDS = 2


# Initialize OpenAI client
client = openai.OpenAI(base_url=API_BASE, api_key="dummy")

# Workspace directory
WORKSPACE_BASE_DIR = "workspace"
HTTP_SERVER_PORT = 8100
MAX_PROMPT_CHARS = getattr(api_config, "MAX_PROMPT_CHARS", 16000)
HTTP_SERVER_BASE = (
    f"http://localhost:{HTTP_SERVER_PORT}"  # you can replace localhost to your local ip
)


def get_session_workspace(session_id: str) -> str:
    """è¿”å›æŒ‡å®š session çš„ workspace è·¯å¾„ï¼ˆworkspace/{session_id}/ï¼‰ã€‚"""
    if not session_id:
        session_id = "default"
    session_dir = os.path.join(WORKSPACE_BASE_DIR, session_id)
    os.makedirs(session_dir, exist_ok=True)
    return session_dir


def build_download_url(rel_path: str) -> str:
    try:
        encoded = quote(rel_path, safe="/")
    except Exception:
        encoded = rel_path
    return f"{HTTP_SERVER_BASE}/{encoded}"


# FastAPI app
app = FastAPI()

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


def start_http_server():
    """å¯åŠ¨HTTPæ–‡ä»¶æœåŠ¡å™¨ï¼ˆä¸ä¿®æ”¹å…¨å±€å·¥ä½œç›®å½•ï¼‰ã€‚"""
    os.makedirs(WORKSPACE_BASE_DIR, exist_ok=True)
    handler = partial(
        http.server.SimpleHTTPRequestHandler, directory=WORKSPACE_BASE_DIR
    )
    with socketserver.TCPServer(("", HTTP_SERVER_PORT), handler) as httpd:
        print(f"HTTP Server serving {WORKSPACE_BASE_DIR} at port {HTTP_SERVER_PORT}")
        httpd.serve_forever()


# Start HTTP server in a separate thread
threading.Thread(target=start_http_server, daemon=True).start()


# ä¼šè¯çº§åˆ«çš„ä¸­æ–­æ ‡è®°
SESSION_STOP_FLAGS: Dict[str, bool] = defaultdict(bool)
session_flag_lock = threading.Lock()


def trigger_stop_flag(session_id: str) -> None:
    with session_flag_lock:
        SESSION_STOP_FLAGS[session_id or "default"] = True


def reset_stop_flag(session_id: str) -> None:
    with session_flag_lock:
        SESSION_STOP_FLAGS[session_id or "default"] = False


def should_stop(session_id: str) -> bool:
    with session_flag_lock:
        return SESSION_STOP_FLAGS.get(session_id or "default", False)


def collect_file_info(directory: str) -> str:
    """æ”¶é›†æ–‡ä»¶ä¿¡æ¯"""
    all_file_info_str = ""
    dir_path = Path(directory)
    if not dir_path.exists():
        return ""

    files = sorted([f for f in dir_path.iterdir() if f.is_file()])
    for idx, file_path in enumerate(files, start=1):
        size_bytes = os.path.getsize(file_path)
        size_kb = size_bytes / 1024
        size_str = f"{size_kb:.1f}KB"
        file_info = {"name": file_path.name, "size": size_str}
        file_info_str = json.dumps(file_info, indent=4, ensure_ascii=False)
        all_file_info_str += f"File {idx}:\n{file_info_str}\n\n"
    return all_file_info_str


def format_workspace_payload(workspace_payload: list[dict]) -> str:
    """å°†å‰ç«¯ä¼ å…¥çš„ workspace æ–‡ä»¶å…ƒä¿¡æ¯è½¬æ¢ä¸º prompt æ–‡æœ¬ã€‚"""
    formatted = []
    for idx, entry in enumerate(workspace_payload, start=1):
        info = {k: v for k, v in entry.items() if v is not None}
        download_url = info.get("download_url")
        info = {
            "name": entry.get("name"),
            "extension": entry.get("extension"),
        }
        size_value = entry.get("size")
        if isinstance(size_value, (int, float)):
            info["size"] = f"{size_value / 1024:.1f}KB"
        elif size_value:
            info["size"] = size_value
        download_url = entry.get("download_url")
        if download_url:
            info["download_url"] = download_url
        formatted.append(
            f"File {idx}:\n" + json.dumps(info, indent=4, ensure_ascii=False) + "\n\n"
        )
    return "".join(formatted).strip()


def iter_sqlite_files(workspace_dir: Path) -> list[Path]:
    """é€’å½’æšä¸¾ workspace å†…çš„ SQLite æ–‡ä»¶åˆ—è¡¨ã€‚"""
    if not isinstance(workspace_dir, Path):
        workspace_dir = Path(workspace_dir)
    found: dict[str, Path] = {}
    for pattern in SQLITE_PATTERNS:
        try:
            for file in workspace_dir.rglob(pattern):
                if file.is_file():
                    found[str(file.resolve())] = file
        except Exception:
            continue
    return sorted(found.values())


def summarize_sqlite_schema(workspace_dir: Path) -> str:
    """éå† workspace ä¸‹çš„ SQLite æ–‡ä»¶å¹¶è¿”å›è¡¨ä¸å­—æ®µæ‘˜è¦ã€‚"""
    summaries: list[str] = []
    for db_file in iter_sqlite_files(workspace_dir):
        try:
            conn = sqlite3.connect(db_file)
            cursor = conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = [row[0] for row in cursor.fetchall()]
            for table in tables:
                cursor.execute(f"PRAGMA table_info('{table}')")
                columns = [col[1] for col in cursor.fetchall()]
                column_desc = ", ".join(columns) if columns else "(æ— å­—æ®µ)"
                summaries.append(f"{db_file.name}:{table} => {column_desc}")
            conn.close()
        except Exception as exc:
            summaries.append(f"{db_file.name} è¯»å–å¤±è´¥: {exc}")
    return "\n".join(summaries).strip()


def list_sqlite_tables(workspace_dir: Path) -> set[str]:
    """è¿”å› workspace å†…æ‰€æœ‰ sqlite æ–‡ä»¶ä¸­å‡ºç°çš„è¡¨åé›†åˆã€‚"""
    tables: set[str] = set()
    for db_file in iter_sqlite_files(workspace_dir):
        try:
            conn = sqlite3.connect(db_file)
            cursor = conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables.update(row[0] for row in cursor.fetchall() if row and row[0])
            conn.close()
        except Exception:
            continue
    return tables


TABLE_TOKEN_PATTERN = re.compile(r"[A-Za-z_][A-Za-z0-9_]*")
SQL_TABLE_PATTERN = re.compile(
    r"(?:from|join|into|update|table)\s+([A-Za-z_][A-Za-z0-9_]*)",
    re.IGNORECASE,
)
SQL_PRAGMA_PATTERN = re.compile(
    r"pragma\s+table_info\s*\(\s*['\"]?([A-Za-z_][A-Za-z0-9_]*)",
    re.IGNORECASE,
)


def extract_table_mentions_from_text(
    text: str, known_tables: set[str]
) -> tuple[set[str], set[str]]:
    tokens = set(TABLE_TOKEN_PATTERN.findall(text or ""))
    known = {tok for tok in tokens if tok in known_tables}
    unknown = {
        tok
        for tok in tokens
        if tok not in known_tables
        and "_" in tok
        and tok.lower() not in {"sqlite_master", "sqlite_sequence"}
    }
    return known, unknown


def extract_sql_table_names(code: str) -> set[str]:
    tables = set(SQL_TABLE_PATTERN.findall(code or ""))
    tables.update(SQL_PRAGMA_PATTERN.findall(code or ""))
    return tables


def snapshot_workspace_files(directory: str) -> set[str]:
    """ç”Ÿæˆ workspace ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„é›†åˆã€‚"""
    try:
        return {str(p.resolve()) for p in Path(directory).rglob("*") if p.is_file()}
    except Exception:
        return set()


def get_file_icon(extension):
    """è·å–æ–‡ä»¶å›¾æ ‡"""
    ext = extension.lower()
    icons = {
        (".jpg", ".jpeg", ".png", ".gif", ".bmp"): "ğŸ–¼ï¸",
        (".pdf",): "ğŸ“•",
        (".doc", ".docx"): "ğŸ“˜",
        (".txt",): "ğŸ“„",
        (".md",): "ğŸ“",
        (".csv", ".xlsx"): "ğŸ“Š",
        (".json", ".sqlite"): "ğŸ—„ï¸",
        (".mp4", ".avi", ".mov"): "ğŸ¥",
        (".mp3", ".wav"): "ğŸµ",
        (".zip", ".rar", ".tar"): "ğŸ—œï¸",
    }

    for extensions, icon in icons.items():
        if ext in extensions:
            return icon
    return "ğŸ“"


def uniquify_path(target: Path) -> Path:
    """è‹¥ç›®æ ‡å·²å­˜åœ¨ï¼Œç”Ÿæˆ 'name (1).ext'ã€'name (2).ext' å½¢å¼çš„æ–°è·¯å¾„ã€‚"""
    if not target.exists():
        return target
    parent = target.parent
    stem = target.stem
    suffix = target.suffix
    import re as _re

    m = _re.match(r"^(.*) \((\d+)\)$", stem)
    base = stem
    start = 1
    if m:
        base = m.group(1)
        try:
            start = int(m.group(2)) + 1
        except Exception:
            start = 1
    i = start
    while True:
        candidate = parent / f"{base} ({i}){suffix}"
        if not candidate.exists():
            return candidate
        i += 1


def execute_code(code_str):
    """æ‰§è¡ŒPythonä»£ç """
    stdout_capture = io.StringIO()
    stderr_capture = io.StringIO()
    try:
        with contextlib.redirect_stdout(stdout_capture), contextlib.redirect_stderr(
            stderr_capture
        ):
            exec(code_str, {})
        output = stdout_capture.getvalue()
        if stderr_capture.getvalue():
            output += stderr_capture.getvalue()
        return output
    except Exception as exec_error:
        return f"[Error]: {str(exec_error)}"


# API Routes
@app.get("/workspace/files")
async def get_workspace_files(session_id: str = Query("default")):
    """è·å–å·¥ä½œåŒºæ–‡ä»¶åˆ—è¡¨ï¼ˆæ”¯æŒ session éš”ç¦»ï¼‰"""
    workspace_dir = get_session_workspace(session_id)
    generated_dir = Path(workspace_dir) / "generated"
    # è·å– generated ç›®å½•ä¸‹çš„æ–‡ä»¶åé›†åˆ
    generated_files = (
        set(f.name for f in generated_dir.iterdir() if f.is_file())
        if generated_dir.exists()
        else set()
    )

    files = []
    for file_path in Path(workspace_dir).iterdir():
        if file_path.is_file():
            if file_path.name in generated_files:
                continue
            stat = file_path.stat()
            rel_path = f"{session_id}/{file_path.name}"
            files.append(
                {
                    "name": file_path.name,
                    "size": stat.st_size,
                    "extension": file_path.suffix.lower(),
                    "icon": get_file_icon(file_path.suffix),
                    "download_url": build_download_url(rel_path),
                    "preview_url": (
                        build_download_url(rel_path)
                        if file_path.suffix.lower()
                        in [
                            ".jpg",
                            ".jpeg",
                            ".png",
                            ".gif",
                            ".bmp",
                            ".pdf",
                            ".txt",
                            ".doc",
                            ".docx",
                            ".csv",
                            ".xlsx",
                        ]
                        else None
                    ),
                }
            )
    return {"files": files}


# ---------- Workspace Tree & Single File Delete ----------
def _rel_path(path: Path, root: Path) -> str:
    try:
        rel = path.relative_to(root)
        return rel.as_posix()
    except Exception:
        return path.name


def build_tree(path: Path, root: Path | None = None) -> dict:
    if root is None:
        root = path
    node: dict = {
        "name": path.name or "workspace",
        "path": _rel_path(path, root),
        "is_dir": path.is_dir(),
    }
    if path.is_dir():
        children = []

        # è‡ªå®šä¹‰æ’åºï¼šgenerated æ–‡ä»¶å¤¹æ”¾åœ¨æœ€åï¼Œå…¶ä»–æŒ‰ç›®å½•ä¼˜å…ˆã€åç§°æ’åº
        def sort_key(p):
            is_generated = p.name == "generated"
            is_dir = p.is_dir()
            return (is_generated, not is_dir, p.name.lower())

        for child in sorted(path.iterdir(), key=sort_key):
            if child.name.startswith("."):
                continue
            children.append(build_tree(child, root))
        node["children"] = children
    else:
        node["size"] = path.stat().st_size
        node["extension"] = path.suffix.lower()
        node["icon"] = get_file_icon(path.suffix)
        rel = _rel_path(path, root)
        node["download_url"] = build_download_url(rel)
    return node


@app.get("/workspace/tree")
async def workspace_tree(session_id: str = Query("default")):
    workspace_dir = get_session_workspace(session_id)
    root = Path(workspace_dir)
    tree_data = build_tree(root, root)

    # åœ¨ä¸‹è½½é“¾æ¥å‰åŠ ä¸Š session_id å‰ç¼€
    def prefix_urls(node, sid):
        if "download_url" in node and node["download_url"]:
            # é‡æ–°æ„å»ºåŒ…å« session_id çš„è·¯å¾„
            rel = node.get("path", "")
            node["download_url"] = build_download_url(f"{sid}/{rel}")
        if "children" in node:
            for child in node["children"]:
                prefix_urls(child, sid)

    prefix_urls(tree_data, session_id)
    return tree_data


@app.delete("/workspace/file")
async def delete_workspace_file(
    path: str = Query(..., description="relative path under workspace"),
    session_id: str = Query("default"),
):
    workspace_dir = get_session_workspace(session_id)
    abs_workspace = Path(workspace_dir).resolve()
    target = (abs_workspace / path).resolve()
    if abs_workspace not in target.parents and target != abs_workspace:
        raise HTTPException(status_code=400, detail="Invalid path")
    if not target.exists():
        raise HTTPException(status_code=404, detail="Not found")
    if target.is_dir():
        raise HTTPException(status_code=400, detail="Folder deletion not allowed")
    try:
        target.unlink()
        return {"message": "deleted"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/workspace/move")
async def move_path(
    src: str = Query(..., description="relative source path under workspace"),
    dst_dir: str = Query("", description="relative target directory under workspace"),
    session_id: str = Query("default"),
):
    """åœ¨åŒä¸€ workspace å†…ç§»åŠ¨ï¼ˆæˆ–é‡å‘½åï¼‰æ–‡ä»¶/ç›®å½•ã€‚
    - src: æºç›¸å¯¹è·¯å¾„ï¼ˆå¿…å¡«ï¼‰
    - dst_dir: ç›®æ ‡ç›®å½•ï¼ˆç›¸å¯¹è·¯å¾„ï¼Œç©ºè¡¨ç¤ºç§»åŠ¨åˆ°æ ¹ç›®å½•ï¼‰
    """
    workspace_dir = get_session_workspace(session_id)
    abs_workspace = Path(workspace_dir).resolve()

    abs_src = (abs_workspace / src).resolve()
    if abs_workspace not in abs_src.parents and abs_src != abs_workspace:
        raise HTTPException(status_code=400, detail="Invalid src path")
    if not abs_src.exists():
        raise HTTPException(status_code=404, detail="Source not found")

    abs_dst_dir = (abs_workspace / (dst_dir or "")).resolve()
    if abs_workspace not in abs_dst_dir.parents and abs_dst_dir != abs_workspace:
        raise HTTPException(status_code=400, detail="Invalid dst_dir path")
    abs_dst_dir.mkdir(parents=True, exist_ok=True)

    target = abs_dst_dir / abs_src.name
    target = uniquify_path(target)
    try:
        shutil.move(str(abs_src), str(target))
        rel_new = str(target.relative_to(abs_workspace))
        return {"message": "moved", "new_path": rel_new}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Move failed: {e}")


@app.delete("/workspace/dir")
async def delete_workspace_dir(
    path: str = Query(..., description="relative directory under workspace"),
    recursive: bool = Query(True, description="delete directory recursively"),
    session_id: str = Query("default"),
):
    """åˆ é™¤ workspace ä¸‹çš„ç›®å½•ã€‚é»˜è®¤é€’å½’åˆ é™¤ï¼Œç¦æ­¢åˆ é™¤æ ¹ç›®å½•ã€‚"""
    workspace_dir = get_session_workspace(session_id)
    abs_workspace = Path(workspace_dir).resolve()
    target = (abs_workspace / path).resolve()
    if abs_workspace not in target.parents and target != abs_workspace:
        raise HTTPException(status_code=400, detail="Invalid path")
    if target == abs_workspace:
        raise HTTPException(status_code=400, detail="Cannot delete workspace root")
    if not target.exists():
        raise HTTPException(status_code=404, detail="Not found")
    if not target.is_dir():
        raise HTTPException(status_code=400, detail="Not a directory")
    try:
        if recursive:
            shutil.rmtree(target)
        else:
            target.rmdir()
        return {"message": "deleted"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/proxy")
async def proxy(url: str):
    """Simple CORS proxy for previewing external files.
    WARNING: For production, add domain allowlist and authentication.
    """
    try:
        async with httpx.AsyncClient(follow_redirects=True, timeout=15) as client:
            r = await client.get(url)
        return Response(
            content=r.content,
            media_type=r.headers.get("content-type", "application/octet-stream"),
            headers={"Access-Control-Allow-Origin": "*"},
            status_code=r.status_code,
        )
    except Exception as e:
        raise HTTPException(status_code=502, detail=f"Proxy fetch failed: {e}")


@app.post("/workspace/upload")
async def upload_files(
    files: List[UploadFile] = File(...), session_id: str = Query("default")
):
    """ä¸Šä¼ æ–‡ä»¶åˆ°å·¥ä½œåŒºï¼ˆæ”¯æŒ session éš”ç¦»ï¼‰"""
    workspace_dir = get_session_workspace(session_id)
    uploaded_files = []

    for file in files:
        # å”¯ä¸€åŒ–æ–‡ä»¶åï¼Œé¿å…è¦†ç›–
        dst = uniquify_path(Path(workspace_dir) / file.filename)
        with open(dst, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        uploaded_files.append(
            {
                "name": dst.name,
                "size": len(content),
                "path": str(dst.relative_to(Path(workspace_dir))),
            }
        )

    return {
        "message": f"Successfully uploaded {len(uploaded_files)} files",
        "files": uploaded_files,
    }


@app.delete("/workspace/clear")
async def clear_workspace(session_id: str = Query("default")):
    """æ¸…ç©ºå·¥ä½œåŒºï¼ˆæ”¯æŒ session éš”ç¦»ï¼‰"""
    workspace_dir = get_session_workspace(session_id)
    if os.path.exists(workspace_dir):
        shutil.rmtree(workspace_dir)
    os.makedirs(workspace_dir, exist_ok=True)
    return {"message": "Workspace cleared successfully"}


@app.post("/workspace/upload-to")
async def upload_to_dir(
    dir: str = Query("", description="relative directory under workspace"),
    files: List[UploadFile] = File(...),
    session_id: str = Query("default"),
):
    """ä¸Šä¼ æ–‡ä»¶åˆ° workspace ä¸‹çš„æŒ‡å®šå­ç›®å½•ï¼ˆä»…é™å·¥ä½œåŒºå†…ï¼‰ã€‚"""
    workspace_dir = get_session_workspace(session_id)
    abs_workspace = Path(workspace_dir).resolve()
    target_dir = (abs_workspace / dir).resolve()
    if abs_workspace not in target_dir.parents and target_dir != abs_workspace:
        raise HTTPException(status_code=400, detail="Invalid dir path")
    target_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        dst = uniquify_path(target_dir / f.filename)
        try:
            with open(dst, "wb") as buffer:
                content = await f.read()
                buffer.write(content)
            saved.append(
                {
                    "name": dst.name,
                    "size": len(content),
                    "path": str(dst.relative_to(abs_workspace)),
                }
            )
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Save failed: {e}")
    return {"message": f"uploaded {len(saved)}", "files": saved}


@app.post("/execute")
async def execute_code_api(request: dict):
    """æ‰§è¡Œ Python ä»£ç """
    print("ğŸ”¥ Execute API called:", request)  # Debug log

    try:
        code = request.get("code", "")
        session_id = request.get("session_id", "default")
        workspace_dir = get_session_workspace(session_id)

        if not code:
            raise HTTPException(status_code=400, detail="No code provided")

        print(f"Executing code: {code[:100]}...")  # Debug log (first 100 chars)

        # ä½¿ç”¨å­è¿›ç¨‹å®‰å…¨æ‰§è¡Œï¼Œé¿å… GUI/çº¿ç¨‹é—®é¢˜ï¼ˆåœ¨æŒ‡å®š session workspace ä¸­ï¼‰
        result = execute_code_safe(code, workspace_dir)
        print(f"âœ… Execution result: {result[:200]}...")  # Debug log

        return {
            "success": True,
            "result": result,
            "message": "Code executed successfully",
        }

    except Exception as e:
        print(f"âŒ Execution error: {traceback.format_exc()}")  # Debug log
        return {
            "success": False,
            "result": f"Error: {str(e)}",
            "message": "Code execution failed",
        }


def fix_code_block(content):
    def fix_text(text):
        stack = []
        lines = text.splitlines(keepends=True)
        result = []
        for line in lines:
            stripped = line.strip()
            if stripped.startswith("```python"):
                if stack and stack[-1] == "```python":
                    result.append("```\n")
                    stack.pop()
                stack.append("```python")
                result.append(line)
            elif stripped == "```":
                if stack and stack[-1] == "```python":
                    stack.pop()
                result.append(line)
            else:
                result.append(line)
        while stack:
            result.append("```\n")
            stack.pop()
        return "".join(result)

    if isinstance(content, str):
        return fix_text(content)
    elif isinstance(content, tuple):
        text_part = content[0] if content[0] else ""
        return (fix_text(text_part), content[1])
    return content


def fix_tags_and_codeblock(s: str) -> str:
    """
    ä¿®å¤æœªé—­åˆçš„tagsï¼Œå¹¶ç¡®ä¿</Code>åä»£ç å—é—­åˆã€‚
    """
    pattern = re.compile(
        r"<(Analyze|Understand|Code|Execute|Answer)>(.*?)(?:</\1>|(?=$))", re.DOTALL
    )

    # æ‰¾æ‰€æœ‰åŒ¹é…
    matches = list(pattern.finditer(s))
    if not matches:
        return s  # æ²¡æœ‰æ ‡ç­¾ï¼Œç›´æ¥è¿”å›

    # æ£€æŸ¥æœ€åä¸€ä¸ªåŒ¹é…æ˜¯å¦é—­åˆ
    last_match = matches[-1]
    tag_name = last_match.group(1)
    matched_text = last_match.group(0)

    if not matched_text.endswith(f"</{tag_name}>"):
        # æ²¡æœ‰é—­åˆï¼Œè¡¥ä¸Š
        if tag_name == "Code":
            s = fix_code_block(s) + f"\n```\n</{tag_name}>"
        else:
            s += f"\n</{tag_name}>"

    return s


EMOJI_TAG_MAP = {
    "ğŸ”Analyze": "<Analyze>",
    "ğŸ’»Code": "<Code>",
    "âš¡Execute": "<Execute>",
    "ğŸ“File": "<File>",
    "âœ…Answer": "<Answer>",
}

HEADING_TAG_PATTERN = re.compile(
    r"^\s{0,3}#{2,3}\s*(Analyze|Code|Execute|File|Answer)\s*$",
    re.MULTILINE,
)
FILE_TAG_CAPTURE_PATTERN = re.compile(r"<File>(.*?)</File>", re.DOTALL)
FILE_NAME_PATTERN = re.compile(
    r"([\w\-.]+\.(?:csv|tsv|txt|md|json|png|jpg|jpeg|gif|svg|pdf|xlsx|xls|parquet))",
    re.IGNORECASE,
)
FILENAME_SUFFIX_CLEANER = re.compile(r"\s+\(\d+\)$")


def normalize_filename(name: str) -> str:
    """ç»Ÿä¸€æ–‡ä»¶åå¯¹æ¯”ï¼šå»é™¤ (n)/_modified ç­‰åç¼€å¹¶è½¬å°å†™ã€‚"""
    if not name:
        return ""
    name = name.strip()
    try:
        path = Path(name)
        stem = FILENAME_SUFFIX_CLEANER.sub("", path.stem)
        stem = stem.removesuffix("_modified")
        return f"{stem}{path.suffix}".lower()
    except Exception:
        return name.lower()


def extract_file_claims(content: str) -> set[str]:
    """è§£ææ¨¡å‹åœ¨ <File> ä¸­å£°æ˜çš„æ–‡ä»¶åé›†åˆã€‚"""
    claims: set[str] = set()
    if not content:
        return claims
    for block in FILE_TAG_CAPTURE_PATTERN.findall(content):
        for match in FILE_NAME_PATTERN.findall(block):
            normalized = normalize_filename(match)
            if normalized:
                claims.add(normalized)
    return claims


def normalize_model_tags(content: str) -> str:
    """å°†å¸¸è§çš„ emoji æ ‡ç­¾è½¬æ¢ä¸ºæ ‡å‡† <Tag> å½¢å¼ã€‚"""
    if not content:
        return content
    normalized = content
    for emoji_tag, canonical in EMOJI_TAG_MAP.items():
        normalized = normalized.replace(emoji_tag, canonical)
    normalized = HEADING_TAG_PATTERN.sub(lambda m: f"<{m.group(1)}>", normalized)
    return normalized


SQLITE_PATTERNS = ("*.sqlite", "*.db", "*.db3")


def find_primary_sqlite(workspace_path: Path) -> Path | None:
    """åœ¨ workspace ä¸­ï¼ˆé€’å½’ï¼‰å®šä½é¦–ä¸ª sqlite æ–‡ä»¶ã€‚"""
    for pattern in SQLITE_PATTERNS:
        try:
            candidates = sorted(workspace_path.rglob(pattern))
        except Exception:
            candidates = []
        for file in candidates:
            if file.is_file():
                return file
    return None


def build_schema_bootstrap_block(workspace_path: Path) -> str:
    """ç”Ÿæˆé¦–è½®è‡ªåŠ¨åˆ—å‡º sqlite_master çš„æ¨¡æ¿å“åº”ã€‚"""
    db_path = find_primary_sqlite(workspace_path)
    if not db_path:
        return ""
    try:
        rel_path = db_path.resolve().relative_to(workspace_path.resolve())
        db_name = rel_path.as_posix()
    except Exception:
        db_name = db_path.name
    analyze = (
        "<Analyze>\n"
        "ç³»ç»Ÿæ£€æµ‹åˆ°æ¨¡å‹å°šæœªæ­£ç¡®è¿›å…¥é¦–è½®åˆ†æï¼Œå·²è‡ªåŠ¨è¡¥å……ï¼šå½“å‰ç›®æ ‡=åˆ—å‡ºæ‰€æœ‰è¡¨ç»“æ„ï¼Œ"
        "å¹¶åœ¨åŒè½® <Execute> ä¸­æ‰“å° sqlite_master ç»“æœï¼Œä¾›åç»­å¼•ç”¨ã€‚\n"
        "</Analyze>\n"
    )
    query_lines = "\n".join(
        [
            "SELECT name AS table_name, type, sql",
            "FROM sqlite_master",
            "WHERE type IN ('table', 'view');",
        ]
    )
    code = (
        "<Code>\n"
        "```python\n"
        "import sqlite3\n"
        "import pandas as pd\n"
        "\n"
        f'conn = sqlite3.connect(r"{db_name}")\n'
        f'query = """\n{query_lines}\n"""\n'
        "schema_df = pd.read_sql_query(query, conn)\n"
        "print(schema_df)\n"
        "conn.close()\n"
        "```\n"
        "</Code>"
    )
    return analyze + "\n" + code


def run_schema_bootstrap(workspace_path: Path) -> str:
    """æ‰§è¡Œé¦–è½® schema æŸ¥è¯¢å¹¶è¿”å›å®Œæ•´ <Analyze>/<Code>/<Execute> å—ã€‚"""
    block = build_schema_bootstrap_block(workspace_path)
    if not block:
        return ""
    code_match = re.search(r"```python(.*?)```", block, re.DOTALL)
    script = code_match.group(1).strip() if code_match else ""
    if not script:
        return block
    output = execute_code_safe(script, str(workspace_path))
    exe_block = f"\n<Execute>\n```\n{output}\n```\n</Execute>\n"
    file_block = "\n<File>\næš‚æ— æ–‡ä»¶\n</File>\n"
    return f"{block}{exe_block}{file_block}"


def extract_effective_code(code_str: str) -> str:
    """è‹¥ <Code> ä¸­åŒ…è£¹ä¸‰å¼•å·å­—ç¬¦ä¸²ï¼Œæå–å…¶ä¸­çš„å®é™…è„šæœ¬å†…å®¹ã€‚"""
    if not code_str:
        return ""
    for quote in ('"""', "'''"):
        start = code_str.find(quote)
        if start != -1:
            end = code_str.find(quote, start + 3)
            if end != -1:
                inner = code_str[start + 3 : end].strip()
                # å¦‚æœå†…å±‚è„šæœ¬ä»åŒ…å« import / SELECT ç­‰å…³é”®å­—ï¼Œåˆ™è®¤ä¸ºæ˜¯æœ‰æ•ˆè„šæœ¬
                if any(
                    token in inner
                    for token in ["import", "select", "plt.", "sns.", "pd."]
                ):
                    return inner
    return code_str


def bot_stream(messages, workspace, session_id="default"):
    original_cwd = os.getcwd()
    workspace_path = Path(get_session_workspace(session_id)).resolve()
    workspace_path.mkdir(parents=True, exist_ok=True)
    generated_dir = workspace_path / "generated"
    generated_dir.mkdir(parents=True, exist_ok=True)
    reset_stop_flag(session_id)

    if messages and messages[0]["role"] == "assistant":
        messages = messages[1:]

    workspace_file_info = ""
    tracked_paths: set[str] = set()
    if isinstance(workspace, list) and workspace:
        workspace_file_info = format_workspace_payload(workspace)
        for entry in workspace:
            if not isinstance(entry, dict):
                continue
            name = entry.get("name")
            if not name:
                continue
            tracked_paths.add(str((workspace_path / name).resolve()))
    elif isinstance(workspace, str) and workspace:
        workspace_file_info = collect_file_info(workspace)
        tracked_paths = snapshot_workspace_files(workspace)
    else:
        workspace_file_info = collect_file_info(str(workspace_path))
        tracked_paths = snapshot_workspace_files(str(workspace_path))

    if messages and messages[-1]["role"] == "user":
        user_message = messages[-1]["content"]
        if workspace_file_info:
            messages[-1][
                "content"
            ] = f"# Instruction\n{user_message}\n\n# Data\n{workspace_file_info}"
        else:
            messages[-1]["content"] = f"# Instruction\n{user_message}"

    initial_workspace = set(tracked_paths)
    assistant_reply = ""
    finished = False
    exe_output = None
    iteration = 0
    raw_iterations = 0
    max_raw_iterations = MAX_ITERATIONS * 2
    empty_retry = 0
    forced_reason = ""

    last_code_signature = None
    last_analyze_signature = None
    last_execute_signature = None
    schema_confirmed = False
    schema_only_repeat = 0
    execute_rounds = 0
    non_schema_exec_rounds = 0
    answer_requested = False
    answer_waiting_rounds = 0
    known_tables = list_sqlite_tables(workspace_path)
    recent_tables_used: set[str] = set()
    schema_summary_injected = False
    schema_bootstrap_used = False

    def refund_iteration():
        nonlocal iteration
        iteration = max(0, iteration - 1)

    def trim_messages(input_messages: list[dict]) -> list[dict]:
        serialized = "\n".join(
            json.dumps(m, ensure_ascii=False) for m in input_messages
        )
        if len(serialized) <= MAX_PROMPT_CHARS:
            return input_messages
        trimmed: list[dict] = []
        total = 0
        for msg in reversed(input_messages):
            encoded = json.dumps(msg, ensure_ascii=False)
            if total + len(encoded) > MAX_PROMPT_CHARS:
                break
            trimmed.append(msg)
            total += len(encoded)
        trimmed = list(reversed(trimmed))
        lead = [
            {
                "role": "system",
                "content": "å†å²æ¶ˆæ¯è¿‡é•¿ï¼Œå·²æˆªæ–­æ—©æœŸå¯¹è¯ï¼Œè¯·æ ¹æ®ä»ä¿ç•™çš„å†…å®¹ç»§ç»­ã€‚",
            }
        ]
        return lead + trimmed

    while (
        not finished
        and iteration < MAX_ITERATIONS
        and raw_iterations < max_raw_iterations
    ):
        raw_iterations += 1
        iteration += 1
        print(
            f"[bot_stream] session={session_id} iteration={iteration} raw={raw_iterations} starting, messages={len(messages)}"
        )
        safe_messages = trim_messages(messages)

        response = client.chat.completions.create(
            model=MODEL_PATH,
            messages=safe_messages,
            temperature=0.4,
            stream=True,
            extra_body={
                "add_generation_prompt": False,
                "stop_token_ids": [151676, 151645],
                "max_new_tokens": 4096,
            },
        )
        cur_res = ""
        last_finish_reason = None
        for chunk in response:
            if chunk.choices and chunk.choices[0].delta.content is not None:
                delta = chunk.choices[0].delta.content
                cur_res += delta
                assistant_reply += delta
                yield delta
            if chunk.choices and chunk.choices[0].finish_reason:
                last_finish_reason = chunk.choices[0].finish_reason
            if should_stop(session_id):
                stop_msg = "\n<Execute>\n``````\næ£€æµ‹åˆ°åœæ­¢æŒ‡ä»¤ï¼Œæ­£åœ¨å®‰å…¨ç»“æŸå½“å‰è¿­ä»£ã€‚\n```\n</Execute>\n"
                assistant_reply += stop_msg
                yield stop_msg
                forced_reason = "ä»»åŠ¡å·²æ ¹æ®ç”¨æˆ·çš„åœæ­¢æŒ‡ä»¤ç»ˆæ­¢"
                finished = True
                break
            if "</Answer>" in cur_res:
                if non_schema_exec_rounds == 0:
                    messages.append({"role": "assistant", "content": cur_res})
                    warn_msg = "å°šæœªåŸºäºçœŸå®è¡¨æ‰§è¡Œä»»ä½• EDA/å¯è§†åŒ–ã€‚è¯·å…ˆæŒ‰ç…§è¦æ±‚è¿è¡Œ `SELECT *` ç­‰åˆ†æï¼Œå½¢æˆ <Execute>/<File> ç»“æœåå†ç»™å‡º <Answer>ã€‚"
                    messages.append({"role": "user", "content": warn_msg})
                    cur_res = cur_res.replace("<Answer>", "<Answer (ignored)>")
                else:
                    finished = True
                    break

        cur_res = normalize_model_tags(cur_res)
        fixed_res = fix_tags_and_codeblock(cur_res)
        if fixed_res != cur_res:
            extra_text = fixed_res[len(cur_res) :]
            if extra_text:
                assistant_reply += extra_text
                yield extra_text
            cur_res = fixed_res

        print(
            f"[bot_stream] session={session_id} iteration={iteration} finish_reason={last_finish_reason} has_code={'<Code>' in cur_res} closed={'</Code>' in cur_res} len={len(cur_res)}"
        )

        analyze_match = re.search(r"<Analyze>(.*?)</Analyze>", cur_res, re.DOTALL)
        analyze_content = analyze_match.group(1).strip() if analyze_match else ""
        analyze_signature = (
            re.sub(r"\s+", " ", analyze_content) if analyze_content else ""
        )

        if not analyze_content:
            messages.append({"role": "assistant", "content": cur_res})
            if not schema_confirmed and not schema_bootstrap_used:
                auto_block = run_schema_bootstrap(workspace_path)
                if auto_block:
                    schema_bootstrap_used = True
                    schema_confirmed = True
                    latest_tables = list_sqlite_tables(workspace_path)
                    if latest_tables:
                        known_tables = latest_tables
                    assistant_reply += auto_block
                    yield auto_block
                    messages.append({"role": "assistant", "content": auto_block})
                    continue
            analyze_prompt = "ä½ çš„è¾“å‡ºç¼ºå°‘ <Analyze> æ®µï¼Œå¿…é¡»å…ˆåœ¨ <Analyze> ä¸­è¯´æ˜å½“å‰ç›®æ ‡ä¸ä¾æ®ï¼Œå†ç»™å‡º <Code>ã€‚"
            messages.append({"role": "user", "content": analyze_prompt})
            refund_iteration()
            continue

        if (
            schema_confirmed
            and "åˆ—å‡º" in analyze_content
            and "è¡¨ç»“æ„" in analyze_content
        ):
            messages.append({"role": "assistant", "content": cur_res})
            advance_prompt = "è¡¨ç»“æ„å·²åœ¨é¦–è½®åˆ—å‡ºï¼Œè¯·åŸºäºå·²çŸ¥è¡¨/å­—æ®µæå‡ºæ–°çš„åˆ†æç›®æ ‡ï¼Œæ¢ç”¨çœŸå®æŸ¥è¯¢æˆ– EDA ä»»åŠ¡ã€‚"
            messages.append({"role": "user", "content": advance_prompt})
            refund_iteration()
            continue

        if last_analyze_signature and analyze_signature == last_analyze_signature:
            messages.append({"role": "assistant", "content": cur_res})
            diff_prompt = "ä½ çš„ <Analyze> å†…å®¹ä¸ä¸Šä¸€è½®å®Œå…¨ç›¸åŒï¼Œè¯·ç»“åˆæœ€æ–°çš„ <Execute>/<File> ç»“æœæå‡ºä¸åŒçš„åˆ†ææ­¥éª¤ã€‚"
            messages.append({"role": "user", "content": diff_prompt})
            refund_iteration()
            continue

        known_mentions = set()
        unknown_mentions = set()
        require_known_reference = schema_confirmed and non_schema_exec_rounds == 0
        if known_tables:
            known_mentions, unknown_mentions = extract_table_mentions_from_text(
                analyze_content, known_tables
            )
            if schema_confirmed and unknown_mentions:
                messages.append({"role": "assistant", "content": cur_res})
                warn_unknown = (
                    "æ£€æµ‹åˆ°ä½ å¼•ç”¨äº†ä¸å­˜åœ¨äºå®é™… SQLite ä¸­çš„è¡¨ï¼š"
                    + ", ".join(sorted(unknown_mentions))
                    + "ã€‚è¯·é‡æ–°æŸ¥çœ‹ sqlite_master ç»“æœï¼Œä»…ä½¿ç”¨çœŸå®è¡¨åã€‚"
                )
                messages.append({"role": "user", "content": warn_unknown})
                refund_iteration()
                continue
            if require_known_reference and not known_mentions:
                messages.append({"role": "assistant", "content": cur_res})
                table_samples = sorted(known_tables)
                sample_hint = (
                    ", ".join(table_samples[:3]) if table_samples else "çœŸå®è¡¨"
                )
                ref_prompt = (
                    "è¯·åœ¨ <Analyze> ä¸­å¼•ç”¨ sqlite_master è¿”å›çš„çœŸå®è¡¨åï¼ˆå¦‚ï¼š"
                    + sample_hint
                    + "ï¼‰ï¼Œå¹¶ç»“åˆè¿™äº›è¡¨/å­—æ®µåˆ¶å®šä¸‹ä¸€æ­¥åˆ†æè®¡åˆ’ã€‚"
                )
                messages.append({"role": "user", "content": ref_prompt})
                refund_iteration()
                continue

        last_analyze_signature = analyze_signature

        if not cur_res.strip() and not finished:
            empty_retry += 1
            if empty_retry < 3:
                retry_prompt = (
                    "ä¸Šä¸€è½®ä½ æ²¡æœ‰ä»»ä½•è¾“å‡ºï¼Œè¯·ç»§ç»­æŒ‰ç…§æ—¢å®šè®¡åˆ’è¿›è¡Œåˆ†æï¼Œ"
                    "åŠ¡å¿…ç»™å‡º <Analyze>/<Code>/<Execute> çš„å®Œæ•´å†…å®¹ã€‚"
                )
                messages.append({"role": "user", "content": retry_prompt})
                continue
            forced_reason = "è¿ç»­å¤šè½®æœªè¿”å›æ–°å¢å†…å®¹ï¼Œå·²ç»ˆæ­¢æœ¬è½®è¿­ä»£"
            finished = True
            break
        else:
            empty_retry = 0

        if finished:
            break

        has_code_block = "<Code>" in cur_res and "</Code>" in cur_res

        if not has_code_block:
            messages.append({"role": "assistant", "content": cur_res})
            if answer_requested:
                answer_waiting_rounds += 1
                reminder = (
                    "ä½ å·²å®Œæˆå¿…è¦çš„ä»£ç æ‰§è¡Œï¼Œè¯·ç›´æ¥ç»™å‡º <Answer>ï¼Œæ€»ç»“ <Execute>/<File> çš„å‘ç°å¹¶æå‡ºå»ºè®®ï¼Œ"
                    "ä¸è¦å†ç»™æ–°çš„ <Analyze>/<Code>ã€‚"
                )
                messages.append({"role": "user", "content": reminder})
                if answer_waiting_rounds >= 2:
                    violation_block = "\n<Answer>\nå¤šæ¬¡æé†’åä»æœªè¾“å‡º <Answer>ï¼Œä»»åŠ¡è¢«ç»ˆæ­¢ã€‚è¯·ä½¿ç”¨ç°æœ‰ç»“æœè‡ªè¡Œæ€»ç»“æˆ–é‡æ–°å‘èµ·ä»»åŠ¡ã€‚\n</Answer>\n"
                    assistant_reply += violation_block
                    yield violation_block
                    return
            else:
                code_prompt = (
                    "ä½ çš„è¾“å‡ºç¼ºå°‘ <Code> æ®µã€‚è¯·åœ¨ <Analyze> åç«‹åˆ»æä¾›å®Œæ•´çš„ Python ä»£ç ï¼ˆå« import/è¿æ¥/EDA/plt ä¿å­˜/conn.close()ï¼‰ï¼Œ"
                    "ä»¥ä¾¿ç³»ç»Ÿæ‰§è¡Œã€‚"
                )
                messages.append({"role": "user", "content": code_prompt})
            refund_iteration()
            continue

        if last_finish_reason in {"stop", "length"} and not finished:
            if "<Code>" in cur_res and "</Code>" not in cur_res:
                missing_tag = "</Code>"
                cur_res += missing_tag
                assistant_reply += missing_tag
                yield missing_tag
            elif "<Code>" not in cur_res:
                # æ¨¡å‹æœªè¾“å‡º <Code>ï¼Œå‘å…¶è¿½åŠ çº é”™æç¤ºå¹¶è¿›å…¥ä¸‹ä¸€è½®
                messages.append({"role": "assistant", "content": cur_res})
                correction_prompt = (
                    "ä½ å¿…é¡»ä¸¥æ ¼æŒ‰å¦‚ä¸‹ç»“æ„è¾“å‡ºï¼šå…ˆç”¨ <Analyze> æ‹†è§£ä»»åŠ¡ï¼Œç´§æ¥ç€åœ¨ <Code> ä¸­ç»™å‡ºå¯æ‰§è¡Œçš„"
                    " Python ä»£ç ï¼ˆä½¿ç”¨ ```python ... ``` åŒ…è£¹ï¼‰ï¼Œç­‰å¾…ç³»ç»Ÿæ‰§è¡Œï¼Œå†ç»“åˆ <Execute>/<File> ç»“æœ"
                    " ç»§ç»­åˆ†æã€‚ä¸è¦é‡å¤æ¬¢è¿è¯­ï¼Œç«‹åˆ»è¡¥å……ç¼ºå¤±çš„ <Code>ã€‚"
                )
                messages.append({"role": "user", "content": correction_prompt})
                continue

        claimed_files_in_round = extract_file_claims(cur_res)

        if "</Code>" in cur_res and not finished:
            if answer_requested:
                messages.append({"role": "assistant", "content": cur_res})
                reminder = "åˆ†æå·²å®Œæˆï¼Œè¯·åœæ­¢è¾“å‡ºæ–°çš„ <Code>ã€‚åœ¨ä¸‹ä¸€è½®ç›´æ¥ç¼–å†™ <Answer>ï¼Œæ€»ç»“å·²å¾—åˆ°çš„ <Execute>/<File> ç»“æœå¹¶ç»™å‡ºè¿›ä¸€æ­¥å»ºè®®ã€‚"
                messages.append({"role": "user", "content": reminder})
                answer_waiting_rounds += 1
                if answer_waiting_rounds >= 2:
                    violation_block = "\n<Answer>\nå¤šæ¬¡æé†’åä»æœªè¾“å‡º <Answer>ï¼Œä»»åŠ¡è¢«è‡ªåŠ¨ç»ˆæ­¢ã€‚è¯·ä½¿ç”¨ç°æœ‰ <Execute>/<File> ç»“æœæ‰‹åŠ¨æ€»ç»“æˆ–é‡æ–°å‘èµ·æŒ‡ä»¤ã€‚\n</Answer>\n"
                    assistant_reply += violation_block
                    yield violation_block
                    return
                continue
            messages.append({"role": "assistant", "content": cur_res})
            code_match = re.search(r"<Code>(.*?)</Code>", cur_res, re.DOTALL)
            if code_match:
                code_content = code_match.group(1).strip()
                md_match = re.search(r"```(?:python)?(.*?)```", code_content, re.DOTALL)
                code_str = md_match.group(1).strip() if md_match else code_content
                effective_code = extract_effective_code(code_str)

                code_signature = "\n".join(
                    line.strip() for line in effective_code.splitlines()
                ).strip()
                normalized_code = effective_code.lower()
                if code_signature and code_signature == last_code_signature:
                    reminder = (
                        "ä½ çš„ä»£ç ä¸ä¸Šä¸€è½®å®Œå…¨ç›¸åŒã€‚è¯·æ ¹æ®å·²è·å–çš„è¡¨ç»“æ„æ¨è¿›æ–°çš„åˆ†æï¼Œ"
                        "ä¸è¦é‡å¤åˆ—å‡º sqlite_masterã€‚"
                    )
                    messages.append({"role": "user", "content": reminder})
                    refund_iteration()
                    continue
                if (
                    schema_confirmed
                    and "sqlite_master" in normalized_code
                    and "pragma" not in normalized_code
                ):
                    schema_only_repeat += 1
                    table_examples = sorted(known_tables)
                    example_text = (
                        ", ".join(table_examples[:3]) if table_examples else "çœŸå®è¡¨"
                    )
                    sample_next = (
                        f"ä¾‹å¦‚ï¼šSELECT * FROM {table_examples[0]} LIMIT 5"
                        if table_examples
                        else "ä¾‹å¦‚ï¼šSELECT * FROM æŸä¸ªçœŸå®è¡¨ LIMIT 5"
                    )
                    refresh_prompt = (
                        "è¡¨ç»“æ„å·²ç»æ˜ç¡®ï¼Œæ— éœ€å†æ¬¡æŸ¥è¯¢ sqlite_masterã€‚è¯·ç›´æ¥å¯¹çœŸå®è¡¨ï¼ˆå¦‚ï¼š"
                        + example_text
                        + f"ï¼‰æ‰§è¡Œ SELECT/EDAï¼Œæ¯”å¦‚ {sample_next} æˆ–ç»˜åˆ¶å¯¹åº”å­—æ®µçš„åˆ†å¸ƒã€‚"
                    )
                    if schema_only_repeat >= 2:
                        violation_block = (
                            "\n<Answer>\nå·²ç¡®è®¤è¡¨ç»“æ„åä»è¿ç»­è¾“å‡º sqlite_master æŸ¥è¯¢ï¼Œä»»åŠ¡è¢«è‡ªåŠ¨ç»ˆæ­¢ã€‚"
                            " è¯·é‡æ–°å‘èµ·ä¼šè¯ï¼Œå¹¶åœ¨é¦–è½®ä¹‹å¤–ç›´æ¥é’ˆå¯¹çœŸå®è¡¨æ‰§è¡Œ SELECT/EDAã€‚\n</Answer>\n"
                        )
                        assistant_reply += violation_block
                        yield violation_block
                        return
                    messages.append({"role": "user", "content": refresh_prompt})
                    refund_iteration()
                    continue
                else:
                    schema_only_repeat = 0
                sql_tables_used = extract_sql_table_names(effective_code)
                if sql_tables_used:
                    recent_tables_used = sql_tables_used
                invalid_tables = set()
                if known_tables and sql_tables_used:
                    invalid_tables = {
                        tbl
                        for tbl in sql_tables_used
                        if tbl not in known_tables
                        and tbl.lower() not in {"sqlite_master", "sqlite_sequence"}
                    }

                post_execute_prompts: list[str] = []
                if schema_confirmed and invalid_tables:
                    invalid_msg = (
                        "è„šæœ¬ä¸­å¼•ç”¨äº†ç³»ç»Ÿå°šæœªç¡®è®¤çš„è¡¨ï¼š"
                        + ", ".join(sorted(invalid_tables))
                        + "ã€‚ç³»ç»Ÿä¼šå°è¯•æ‰§è¡Œä»¥éªŒè¯å…¶æ˜¯å¦çœŸå®å­˜åœ¨ï¼›è‹¥ä¸‹ä¸€è½® <Execute> æŠ¥é”™ï¼Œè¯·ä¼˜å…ˆå›åˆ° sqlite_master/PRAGMA é‡æ–°æ ¸å¯¹è¡¨åã€‚"
                    )
                    post_execute_prompts.append(invalid_msg)

                if not schema_confirmed and "sqlite_master" not in normalized_code:
                    schema_prompt = (
                        "è¯·å…ˆåœ¨ <Code> ä¸­æ‰§è¡Œ `SELECT name FROM sqlite_master WHERE type='table'` å¹¶åˆ—å‡ºçœŸå®è¡¨ç»“æ„ï¼Œ"
                        "é¦–è½®å¿…é¡»å®Œæˆè¡¨ç»“æ„ç¡®è®¤åæ‰èƒ½ç»§ç»­ EDAã€‚"
                    )
                    messages.append({"role": "user", "content": schema_prompt})
                    refund_iteration()
                    continue

                missing_imports = []
                if (
                    "pd." in effective_code
                    and "import pandas as pd" not in effective_code
                ):
                    missing_imports.append("import pandas as pd")
                if (
                    "plt." in effective_code
                    and "import matplotlib.pyplot as plt" not in effective_code
                ):
                    missing_imports.append("import matplotlib.pyplot as plt")
                if (
                    "sns." in effective_code
                    and "import seaborn as sns" not in effective_code
                ):
                    missing_imports.append("import seaborn as sns")
                if missing_imports:
                    import_prompt = (
                        "æ£€æµ‹åˆ° <Code> ä½¿ç”¨äº† pandas/matplotlib/seabornï¼Œä½†ç¼ºå°‘ä»¥ä¸‹å¯¼å…¥ï¼š"
                        + ", ".join(missing_imports)
                        + "ã€‚è¯·è¡¥å…¨å¯¼å…¥åå†æ‰§è¡Œã€‚"
                    )
                    messages.append({"role": "user", "content": import_prompt})
                    refund_iteration()
                    continue

                if "import sqlite3" not in effective_code:
                    sqlite_prompt = (
                        "æ¯ä¸ª <Code> è„šæœ¬éƒ½éœ€æ˜¾å¼ `import sqlite3` å¹¶å»ºç«‹æ•°æ®åº“è¿æ¥ã€‚"
                        " è¯·å°†å®Œæ•´è„šæœ¬è¡¥å…¨ï¼ˆå« import / connect / æ‰§è¡Œ / closeï¼‰åå†è¿è¡Œã€‚"
                    )
                    messages.append({"role": "user", "content": sqlite_prompt})
                    refund_iteration()
                    continue

                if "sqlite3.connect" not in effective_code:
                    connect_prompt = (
                        "æ£€æµ‹åˆ°ä»£ç ç¼ºå°‘ `sqlite3.connect(...)`ï¼Œè€Œæœ¬ç³»ç»Ÿæ¯æ¬¡æ‰§è¡Œéƒ½ä¼šåœ¨ç‹¬ç«‹è¿›ç¨‹è¿è¡Œï¼Œ"
                        "ä¸èƒ½å¤ç”¨ä¸Šä¸€è½®è¿æ¥ã€‚è¯·åœ¨ <Code> ä¸­åˆ›å»ºå¹¶å…³é—­è¿æ¥åé‡æ–°æäº¤ã€‚"
                    )
                    messages.append({"role": "user", "content": connect_prompt})
                    refund_iteration()
                    continue

                last_code_signature = code_signature

                print(
                    f"[bot_stream] session={session_id} iteration={iteration} executing code, length={len(code_str)}"
                )
                try:
                    before_state = {
                        p.resolve(): (p.stat().st_size, p.stat().st_mtime_ns)
                        for p in workspace_path.rglob("*")
                        if p.is_file()
                    }
                except Exception:
                    before_state = {}

                exe_output = execute_code_safe(
                    effective_code or code_str, str(workspace_path)
                )

                is_schema_code = (
                    "sqlite_master" in normalized_code
                    and "pragma" not in normalized_code
                )

                if not schema_confirmed and "sqlite_master" in normalized_code:
                    schema_confirmed = True
                    latest_tables = list_sqlite_tables(workspace_path)
                    if latest_tables:
                        known_tables = latest_tables
                    if not schema_summary_injected:
                        schema_hint = summarize_sqlite_schema(workspace_path)
                        if schema_hint:
                            schema_summary = (
                                "ç³»ç»Ÿå·²ä»å®é™… sqlite_master/PRAGMA ä¸­è§£æåˆ°ä»¥ä¸‹è¡¨ç»“æ„ï¼Œè¯·åœ¨åç»­ <Analyze>/<Code> ä¸­"
                                " ç›´æ¥å¼•ç”¨è¿™äº›çœŸå®åå­—ï¼Œå¹¶æŒ‰å…¶ä¸­å­—æ®µæ¨è¿›åˆ†æï¼š\n"
                                f"{schema_hint}\n"
                                "ä¸‹ä¸€æ­¥å»ºè®®ï¼šä»ä¸Šè¿°è¡¨ä¸­ä»»é€‰ä¸€ä¸ªï¼ˆå¦‚ç¬¬ä¸€å¼ è¡¨ï¼‰æ‰§è¡Œ `SELECT * ... LIMIT 50` åšåˆæ­¥æ¦‚è§ˆã€‚"
                            )
                            messages.append({"role": "user", "content": schema_summary})
                        schema_summary_injected = True

                try:
                    after_state = {
                        p.resolve(): (p.stat().st_size, p.stat().st_mtime_ns)
                        for p in workspace_path.rglob("*")
                        if p.is_file()
                    }
                except Exception:
                    after_state = {}

                added_paths = [p for p in after_state.keys() if p not in before_state]
                modified_paths = [
                    p
                    for p in after_state.keys()
                    if p in before_state and after_state[p] != before_state[p]
                ]

                artifact_paths = []
                generated_dir_str = str(generated_dir.resolve())
                for p in added_paths:
                    try:
                        if not str(p).startswith(generated_dir_str):
                            dest_path = uniquify_path(generated_dir / p.name)
                            shutil.copy2(p, dest_path)
                            artifact_paths.append(dest_path.resolve())
                        else:
                            artifact_paths.append(p.resolve())
                    except Exception as e:
                        print(f"Error moving file {p}: {e}")

                for p in modified_paths:
                    try:
                        dest_path = uniquify_path(
                            generated_dir / f"{p.stem}_modified{p.suffix}"
                        )
                        shutil.copy2(p, dest_path)
                        artifact_paths.append(dest_path.resolve())
                    except Exception as e:
                        print(f"Error copying modified file {p}: {e}")

                exe_str = f"\n<Execute>\n```\n{exe_output}\n```\n</Execute>\n"
                actual_files = {
                    normalize_filename(Path(p).name) for p in artifact_paths
                }
                file_block_lines = ["<File>"]
                if artifact_paths:
                    for p in artifact_paths:
                        try:
                            rel = p.resolve().relative_to(workspace_path).as_posix()
                        except Exception:
                            rel = p.name
                        url = build_download_url(f"{session_id}/{rel}")
                        name = p.name
                        file_block_lines.append(f"- [{name}]({url})")
                        if p.suffix.lower() in [
                            ".png",
                            ".jpg",
                            ".jpeg",
                            ".gif",
                            ".webp",
                            ".svg",
                        ]:
                            file_block_lines.append(f"![{name}]({url})")
                else:
                    file_block_lines.append("æš‚æ— æ–‡ä»¶")
                file_block_lines.append("</File>")
                file_block = "\n" + "\n".join(file_block_lines) + "\n"

                full_execution_block = exe_str + file_block
                assistant_reply += full_execution_block
                yield full_execution_block
                messages.append({"role": "execute", "content": f"{exe_output}"})
                if claimed_files_in_round:
                    unmatched_claims = sorted(
                        claim
                        for claim in claimed_files_in_round
                        if claim not in actual_files
                    )
                    if unmatched_claims:
                        warn_missing_file = (
                            "ç³»ç»Ÿæœªæ£€æµ‹åˆ°ä½ åœ¨ <File> ä¸­å£°æ˜çš„æ–‡ä»¶ï¼š"
                            + ", ".join(unmatched_claims)
                            + "ã€‚è¯·ç¡®ä¿è„šæœ¬çœŸå®å†™å…¥è¿™äº›æ–‡ä»¶ï¼Œå¹¶ä¾èµ–ç³»ç»Ÿè‡ªåŠ¨è¾“å‡ºçš„ <File> æ®µï¼Œè€Œä¸æ˜¯æ‰‹åŠ¨æœæ’°ã€‚"
                        )
                        messages.append({"role": "user", "content": warn_missing_file})
                for prompt in post_execute_prompts:
                    messages.append({"role": "user", "content": prompt})

                execute_rounds += 1
                if not is_schema_code:
                    non_schema_exec_rounds += 1
                if answer_requested:
                    answer_waiting_rounds = 0
                if (
                    execute_rounds >= ANSWER_MIN_EXEC_ROUNDS
                    and non_schema_exec_rounds >= ANSWER_MIN_NON_SCHEMA_ROUNDS
                    and not answer_requested
                ):
                    answer_requested = True
                    answer_waiting_rounds = 0
                    answer_prompt = (
                        "ä½ å·²å®Œæˆè‡³å°‘ä¸¤è½®ä»£ç æ‰§è¡Œã€‚è¯·åœæ­¢ç»§ç»­ç¼–å†™ <Code>ï¼Œåœ¨ä¸‹ä¸€è½®ç›´æ¥è¾“å‡º <Answer>ï¼Œ"
                        "æ€»ç»“ä¸Šè¿° <Execute>/<File> ç»“æœå¹¶ç»™å‡ºåç»­å»ºè®®ã€‚"
                    )
                    messages.append({"role": "user", "content": answer_prompt})

                exe_signature = (
                    re.sub(r"\s+", " ", exe_output.strip())
                    if isinstance(exe_output, str)
                    else ""
                )
                if (
                    schema_confirmed
                    and exe_signature
                    and last_execute_signature
                    and exe_signature == last_execute_signature
                ):
                    repeat_prompt = "è¿ç»­ä¸¤è½®çš„ <Execute> è¾“å‡ºå®Œå…¨ä¸€è‡´ï¼ˆä»åœ¨åˆ—å‡º sqlite_master ç»“æœï¼‰ã€‚è¯·ç«‹å³æ”¹ç”¨çœŸå®è¡¨è¿›è¡Œ `SELECT *` æˆ–ç»Ÿè®¡åˆ†æã€‚"
                    messages.append({"role": "user", "content": repeat_prompt})
                last_execute_signature = exe_signature or last_execute_signature

                normalized_output = (
                    exe_output.lower() if isinstance(exe_output, str) else ""
                )
                if "no such table" in normalized_output:
                    missing_match = re.search(
                        r"no such table:?\s*([\w\d_]+)", exe_output, re.IGNORECASE
                    )
                    missing_table = missing_match.group(1) if missing_match else ""
                    schema_hint = summarize_sqlite_schema(workspace_path)
                    hint_lines = [
                        "æ‰§è¡Œç»“æœæ˜¾ç¤ºå¼•ç”¨äº†ä¸å­˜åœ¨çš„è¡¨ã€‚è¯·å¤æŸ¥ sqlite_master è¾“å‡ºï¼Œåœ¨ä¸‹ä¸€è½® <Analyze> ä¸­è¯´æ˜ä¿®å¤è®¡åˆ’ï¼Œå¹¶æ”¹ç”¨çœŸå®è¡¨åã€‚"
                    ]
                    if missing_table:
                        hint_lines.append(f"ç¼ºå¤±çš„è¡¨ï¼š{missing_table}")
                    if schema_hint:
                        hint_lines.append(
                            "å½“å‰ workspace ä¸­ SQLite è¡¨ç»“æ„ï¼ˆç³»ç»Ÿå®æ—¶æ‰«æï¼‰ï¼š"
                        )
                        hint_lines.append(schema_hint)
                    messages.append(
                        {"role": "user", "content": "\n\n".join(hint_lines)}
                    )

                current_files = {
                    str(p.resolve()) for p in workspace_path.rglob("*") if p.is_file()
                }
                new_files = current_files - initial_workspace
                if new_files:
                    initial_workspace.update(new_files)

        if should_stop(session_id) and not forced_reason:
            forced_reason = "ä»»åŠ¡å·²æ ¹æ®ç”¨æˆ·çš„åœæ­¢æŒ‡ä»¤ç»ˆæ­¢"
            finished = True
            break

    if not finished and forced_reason == "" and iteration >= MAX_ITERATIONS:
        forced_reason = f"å·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆ{MAX_ITERATIONS}ï¼‰ï¼Œè‡ªåŠ¨ç»“æŸå½“å‰ä»»åŠ¡"

    if forced_reason and "</Answer>" not in assistant_reply:
        answer_block = f"\n<Answer>\n{forced_reason}ã€‚è¯·å‚è€ƒä»¥ä¸Š <Execute>/<File> è¾“å‡ºï¼Œå¿…è¦æ—¶é‡æ–°å‘èµ·æŒ‡ä»¤ã€‚\n</Answer>\n"
        assistant_reply += answer_block
        yield answer_block

    os.chdir(original_cwd)


@app.post("/chat/completions")
async def chat(body: dict = Body(...)):
    messages = body.get("messages", [])
    workspace = body.get("workspace", [])
    session_id = body.get("session_id", "default")

    def generate():
        for delta_content in bot_stream(messages, workspace, session_id):
            # print(delta_content)
            chunk = {
                "id": "chatcmpl-stream",
                "object": "chat.completion.chunk",  # æ ‡è¯†ä¸ºæµå¼å—
                "created": 1677652288,
                "model": MODEL_PATH,
                "choices": [
                    {
                        "index": 0,
                        # 3. ä½¿ç”¨ delta å­—æ®µè€Œé message å­—æ®µ
                        "delta": {
                            "content": delta_content  # ç›´æ¥å¡«å…¥åŸå§‹å†…å®¹ï¼Œä¸è¦è°ƒç”¨ fix_tags
                        },
                        "finish_reason": None,  # ä¼ è¾“ä¸­ä¸º None
                    }
                ],
            }

            yield json.dumps(chunk) + "\n"
            # 5. å¾ªç¯ç»“æŸåï¼Œå‘é€ä¸€ä¸ªç»“æŸæ ‡è®° (Optional, ä½†æ¨è)
        end_chunk = {
            "id": "chatcmpl-stream",
            "object": "chat.completion.chunk",
            "created": 1677652288,
            "model": MODEL_PATH,
            "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}],
        }
        yield json.dumps(end_chunk) + "\n"

    return StreamingResponse(generate(), media_type="text/plain")


@app.post("/chat/stop")
async def stop_chat(body: dict = Body(...)):
    """æ¥æ”¶å‰ç«¯åœæ­¢è¯·æ±‚ï¼Œè®¾ç½®å¯¹åº” session çš„ä¸­æ–­æ ‡è®°ã€‚"""
    session_id = body.get("session_id", "default")
    trigger_stop_flag(session_id)
    return {"message": f"stop signal sent for {session_id}"}


# -------- Export Report (PDF + MD) --------
from datetime import datetime


def _extract_sections_from_messages(messages: list[dict]) -> str:
    """ä»å†å²æ¶ˆæ¯ä¸­æŠ½å– <Answer>..</Answer> ä½œä¸ºæŠ¥å‘Šä¸»ä½“ï¼Œå…¶ä½™éƒ¨åˆ†æŒ‰åŸå§‹é¡ºåºä½œä¸º Appendix æ‹¼æˆ Markdownã€‚"""
    if not isinstance(messages, list):
        return ""
    import re as _re

    parts: list[str] = []
    appendix: list[str] = []

    tag_pattern = r"<(Analyze|Understand|Code|Execute|File|Answer)>([\s\S]*?)</\1>"

    for idx, m in enumerate(messages, start=1):
        role = (m or {}).get("role")
        if role != "assistant":
            continue
        content = str((m or {}).get("content") or "")

        step = 1
        # æŒ‰ç…§åœ¨æ–‡æœ¬ä¸­çš„å‡ºç°é¡ºåºä¾æ¬¡æå–
        for match in _re.finditer(tag_pattern, content, _re.DOTALL):
            tag, seg = match.groups()
            seg = seg.strip()
            if tag == "Answer":
                parts.append(f"{seg}\n")

            appendix.append(f"\n### Step {step}: {tag}\n\n{seg}\n")
            step += 1

    final_text = "".join(parts).strip()
    if appendix:
        final_text += (
            "\n\n\\newpage\n\n# Appendix: Detailed Process\n"
            + "".join(appendix).strip()
        )

    # print(final_text)
    return final_text


def _save_md(md_text: str, base_name: str, workspace_dir: str) -> Path:
    Path(workspace_dir).mkdir(parents=True, exist_ok=True)
    md_path = uniquify_path(Path(workspace_dir) / f"{base_name}.md")
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(md_text)
    return md_path


import pypandoc


def _save_pdf(md_text: str, base_name: str, workspace_dir: str) -> Path:
    Path(workspace_dir).mkdir(parents=True, exist_ok=True)
    pdf_path = uniquify_path(Path(workspace_dir) / f"{base_name}.pdf")

    output = pypandoc.convert_text(
        md_text,
        "pdf",
        format="md",
        outputfile=str(pdf_path),
        extra_args=[
            "--standalone",
            "--pdf-engine=xelatex",
        ],
    )
    return pdf_path


from typing import Optional


def _render_md_to_html(md_text: str, title: Optional[str] = None) -> str:
    """ç®€åŒ–ä¸ºå ä½å®ç°ï¼ˆä»…ä¾›æœªæ¥ PDF æ¸²æŸ“ä½¿ç”¨ï¼‰ã€‚å½“å‰ä»…ç”Ÿæˆ MDã€‚"""
    doc_title = (title or "Report").strip() or "Report"
    safe = (md_text or "").replace("<", "&lt;").replace(">", "&gt;")
    return f"<html><head><meta charset='utf-8'><title>{doc_title}</title></head><body><pre>{safe}</pre></body></html>"


def _save_pdf_from_md(html_text: str, base_name: str) -> Path:
    """TODO: æœåŠ¡ç«¯ PDF æ¸²æŸ“æœªå®ç°ã€‚"""
    raise NotImplementedError("TODO: implement server-side PDF rendering")


def _save_pdf_with_chromium(html_text: str, base_name: str) -> Path:
    """TODO: ä½¿ç”¨ Chromium æ¸²æŸ“ PDFï¼ˆæš‚ä¸å®ç°ï¼‰ã€‚"""
    raise NotImplementedError("TODO: chromium-based PDF rendering")


def _save_pdf_from_text(text: str, base_name: str) -> Path:
    """TODO: çº¯æ–‡æœ¬ PDF æ¸²æŸ“ï¼ˆæš‚ä¸å®ç°ï¼‰ã€‚"""
    raise NotImplementedError("TODO: text-based PDF rendering")


@app.post("/export/report")
async def export_report(body: dict = Body(...)):
    """
    æ¥æ”¶å…¨éƒ¨èŠå¤©å†å²ï¼ˆmessages: [{role, content}...]ï¼‰ï¼ŒæŠ½å– <Analyze>..</Analyze> ~ <Answer>..</Answer>
    ä»…ç”Ÿæˆ Markdown æ–‡ä»¶å¹¶ä¿å­˜åˆ° workspaceï¼›PDF æ¸²æŸ“ç•™ä½œ TODOã€‚
    """
    try:
        messages = body.get("messages", [])
        title = (body.get("title") or "").strip()
        session_id = body.get("session_id", "default")
        workspace_dir = get_session_workspace(session_id)

        if not isinstance(messages, list):
            raise HTTPException(status_code=400, detail="messages must be a list")

        md_text = _extract_sections_from_messages(messages)
        if not md_text:
            md_text = (
                "(No <Analyze>/<Understand>/<Code>/<Execute>/<Answer> sections found.)"
            )

        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_title = re.sub(r"[^\w\-_.]+", "_", title) if title else "Report"
        base_name = f"{safe_title}_{ts}" if title else f"Report_{ts}"

        # Save MD into generated/ folder under workspace
        export_dir = os.path.join(workspace_dir, "generated")
        os.makedirs(export_dir, exist_ok=True)

        print(md_text)
        md_path = _save_md(md_text, base_name, export_dir)

        # PDF æš‚ä¸ç”Ÿæˆï¼ˆTODOï¼‰ã€‚
        pdf_path = _save_pdf(md_text, base_name, export_dir)

        result = {
            "message": "exported",
            "md": md_path.name,
            "pdf": pdf_path.name if pdf_path else None,
            "download_urls": {
                "md": build_download_url(f"{session_id}/generated/{md_path.name}"),
                "pdf": (
                    build_download_url(f"{session_id}/generated/{pdf_path.name}")
                    if pdf_path
                    else None
                ),
            },
        }
        return JSONResponse(result)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨åç«¯æœåŠ¡...")
    print(f"   - APIæœåŠ¡: http://localhost:8200")
    print(f"   - æ–‡ä»¶æœåŠ¡: http://localhost:8100")
    uvicorn.run(app, host="0.0.0.0", port=8200)
